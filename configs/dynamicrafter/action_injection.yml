# lightning.pytorch==2.2.1
seed_everything: 42
trainer:
  accelerator: auto
  strategy: auto
  devices: 8
  num_nodes: 1
  precision: bf16-true
  logger:
    class_path: WandbLogger
    init_args:
      project: 'PVG'
      entity: 'pvg-nju' # team name
      tags:
        - 'Res:1024x576'
        - 'Frm:16'
        - 'DCBaseline'
        - 'Ref:gt_action'
        - 'train'
        - 'dataset:openvid'
  callbacks:
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: 'step'
    - class_path: ModelSummary
      init_args:
        max_depth: 3
    - class_path: ModelCheckpoint
      init_args:
        every_n_train_steps: 2000
        save_top_k: -1
        save_last: true
    - src.utils.training.IncrementalCheckpoint
    - src.metrics.callbacks.FVDMetric
    - src.metrics.callbacks.FIDMetric
    - src.metrics.callbacks.MotionMetric
    - src.metrics.callbacks.MAEActionMetric
    - src.metrics.callbacks.ClipV2VMetric
    - src.metrics.callbacks.DINOMetric
    - src.utils.training.WandbVideoLogger
    - src.utils.training.WandbCodeLogger
    - src.metrics.callbacks.SaveSampleMetrics
    - class_path: src.utils.training.SaveVideo
      init_args:
        type: 'generate'
    - class_path: src.utils.training.SaveVideo
      init_args:
        type: 'gt'
  fast_dev_run: null
  max_epochs: null
  min_epochs: null
  max_steps: 200000
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: 2000
  check_val_every_n_epoch: null
  num_sanity_val_steps: 2
  log_every_n_steps: 10
  enable_checkpointing: null
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: 0.5
  gradient_clip_algorithm: null
  deterministic: false
  benchmark: true
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
model:
  class_path: src.projects.dynamicrafter.module.DynamiCrafterAction
  init_args:
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 5e-5
    full_trainable_modules:
#      - 'image_proj_model' # query transformer
      - 'action_proj_model' # action transformer
#      - model.diffusion_model'
#      - 'model.diffusion_model.time_embed'
#      - 'model.diffusion_model.fps_embedding'
#      - 'model.diffusion_model.init_attn'
#      - 'model.diffusion_model.input_blocks'
#      - 'model.diffusion_model.middle_block'
#      - 'model.diffusion_model.output_blocks'
#      - 'model.diffusion_model.out'
      # all SpatialTransformer
#      - model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2
#      - model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2
#      - model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2
#      - model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2
#      - model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2
#      - model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2
#      - model.diffusion_model.middle_block.1.transformer_blocks.0.attn2
#      - model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2
#      - model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2
#      - model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2
#      - model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2
#      - model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2
#      - model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2
#      - model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2
#      - model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2
#      - model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2
      - model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.input_blocks.1.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.input_blocks.2.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.input_blocks.4.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.input_blocks.5.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.input_blocks.7.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.input_blocks.8.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.middle_block.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.output_blocks.3.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.output_blocks.4.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.output_blocks.5.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.output_blocks.6.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.output_blocks.7.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.output_blocks.8.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.output_blocks.9.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.output_blocks.10.1.transformer_blocks.0.attn2.to_v_a
      - model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_k_a
      - model.diffusion_model.output_blocks.11.1.transformer_blocks.0.attn2.to_v_a

    lora_rank: 256
    lora_trainable_modules: []
    freeze_modules:
      - first_stage_model # vae
      - cond_stage_model # text encoder
      - model # unet
      - logvar #
      - embedder # clip image encoder
      - image_proj_model
      - action_embedder # video mae encoder

    eval_pipeline_call_kwargs:
      num_frames: 16
      width: &width 1024
      height: &height 576
      frame_stride: 15
      unconditional_guidance_scale: 2.0
      ref_fusion_type: &fusion 'top1'
      num_inference_steps: 30

    load_model_kwargs:
      pretrained_checkpoint: Doubiiu/DynamiCrafter_1024

  dict_kwargs:
    rescale_betas_zero_snr: true
    parameterization: "v"
    linear_start: 0.00085
    linear_end: 0.012
    num_timesteps_cond: 1
    log_every_t: 200
    timesteps: 1000
    first_stage_key: video
    cond_stage_key: caption
    cond_stage_trainable: false
    image_proj_model_trainable: true
    action_proj_model_trainable: true
    conditioning_key: hybrid
    image_size: [72, 128]
    channels: 4
    scale_by_std: false
    scale_factor: 0.18215
    use_ema: false
    uncond_prob: 0.05
    uncond_type: 'empty_seq'
    rand_cond_frame: true
    use_dynamic_rescale: true
    base_scale: 0.3
    fps_condition_type: 'fps'
    perframe_ae: true

    unet_config:
      target: src.projects.dynamicrafter.DynamiCrafter.lvdm.modules.networks.openaimodel3d.UNetModel
      params:
        in_channels: 8
        out_channels: 4
        model_channels: 320
        attention_resolutions:
          - 4
          - 2
          - 1
        num_res_blocks: 2
        channel_mult:
          - 1
          - 2
          - 4
          - 4
        dropout: 0.1
        num_head_channels: 64
        transformer_depth: 1
        context_dim: 1024
        use_linear: true
        use_checkpoint: false
        temporal_conv: true
        temporal_attention: true
        temporal_self_att_only: true
        use_relative_position: false
        use_causal_attention: false
        temporal_length: 16
        addition_attention: true
        image_cross_attention: true
        action_cross_attention: true
        default_fs: 10
        fs_condition: true

    first_stage_config:
      target: src.projects.dynamicrafter.DynamiCrafter.lvdm.models.autoencoder.AutoencoderKL
      params:
        embed_dim: 4
        monitor: val/rec_loss
        ddconfig:
          double_z: true
          z_channels: 4
          resolution: 256
          in_channels: 3
          out_ch: 3
          ch: 128
          ch_mult:
            - 1
            - 2
            - 4
            - 4
          num_res_blocks: 2
          attn_resolutions: [ ]
          dropout: 0.0
        lossconfig:
          target: torch.nn.Identity

    cond_stage_config:
      target: src.projects.dynamicrafter.DynamiCrafter.lvdm.modules.encoders.condition.FrozenOpenCLIPEmbedder
      params:
        freeze: true
        layer: "penultimate"

    img_cond_stage_config:
      target: src.projects.dynamicrafter.DynamiCrafter.lvdm.modules.encoders.condition.FrozenOpenCLIPImageEmbedderV2
      params:
        freeze: true

    image_proj_stage_config:
      target: src.projects.dynamicrafter.DynamiCrafter.lvdm.modules.encoders.resampler.Resampler
      params:
        dim: 1024
        depth: 4
        dim_head: 64
        heads: 12
        num_queries: 16
        embedding_dim: 1280
        output_dim: 1024
        ff_mult: 4
        video_length: 16

    action_cond_stage_config:
      target: src.projects.condition.VideoMAEEmbedder
      params:
        freeze: true

    action_proj_stage_config:
      target: src.projects.condition.Resampler
      params:
        dim: 1024
        depth: 4
        dim_head: 64
        heads: 12
        num_queries: 25
        embedding_dim: 768
        output_dim: 1024
        ff_mult: 4

    ref_fusion_type: *fusion

data:
  class_path: src.data.VideoDataModule
  init_args:
    video_size:
      - *height
      - *width
    video_length: 16
    # probability of dropping the text prompt
    uncond_text_ratio: 0.0
    uncond_video_ratio: 0.0
    prompt_type: llm
    use_ref_frame: false
    train_annotation_path:
      - datasets/OpenVid-1M/data/openvid-1m.parquet
    val_annotation_path:
      - datasets/OpenVid-1M/data/openvid-1k.parquet
    test_annotation_path:
      - datasets/OpenVid-1M/data/openvid-1k.parquet
    video_dir: datasets
    train_transforms: []
    train_clip_selector:
      class_path: src.data.clip_selector.AllSelector

    val_clip_selector:
      class_path: src.data.clip_selector.RandomSelector
      init_args:
        num_clips: 200
        seed: 42
    test_clip_selector:
      class_path: src.data.clip_selector.AllSelector

    num_workers: 3
    prefetch_factor: 5
    train_batch_size: 1
    val_batch_size: 1
    test_batch_size: 2
    collate_fn: src.data.dataset.collate_fn
    tokenizer_model: null
    sampling_config: # video sampling rate (fps) and its probability, i.e. {4: 0.2, 8: 0.5, 12: 0.3}
      8: 0.33
      10: 0.34
      12: 0.33
    ref_video_type: "gt"
    ref_video_num: 1

#ckpt_path: null
