# lightning.pytorch==2.2.1
seed_everything: 42
trainer:
  accelerator: auto
  strategy: auto
  devices: 8
  num_nodes: 1
  precision: bf16-true
  logger:
    class_path: WandbLogger
    init_args:
      project: 'PVG'
      entity: 'pvg-nju' # team name
      tags:
        - 'Res:720x480'
        - 'Frm:16'
        - 'CogVideoX5B'
        - 'Ref:rag'
        - 'train'
        - 'dataset:openvid'
  callbacks:
    - class_path: LearningRateMonitor
      init_args:
        logging_interval: 'step'
    - class_path: ModelSummary
      init_args:
        max_depth: 3
    - class_path: ModelCheckpoint
      init_args:
        every_n_train_steps: 2000
        save_top_k: -1
        save_last: true
    - src.utils.training.IncrementalCheckpoint
    - src.metrics.callbacks.FVDMetric
    - src.metrics.callbacks.FIDMetric
    - src.metrics.callbacks.MotionMetric
    - src.metrics.callbacks.MAEActionMetric
    - src.metrics.callbacks.ClipV2VMetric
    - src.metrics.callbacks.DINOMetric
    - src.utils.training.WandbVideoLogger
    - src.utils.training.WandbCodeLogger
    - src.metrics.callbacks.SaveSampleMetrics
    - class_path: src.utils.training.SaveVideo
      init_args:
        type: 'generate'
    - class_path: src.utils.training.SaveVideo
      init_args:
        type: 'gt'
  fast_dev_run: null
  max_epochs: null
  min_epochs: null
  max_steps: 200000
  min_steps: null
  max_time: null
  limit_train_batches: null
  limit_val_batches: null
  limit_test_batches: null
  limit_predict_batches: null
  overfit_batches: 0.0
  val_check_interval: 2000
  check_val_every_n_epoch: null
  num_sanity_val_steps: 1
  log_every_n_steps: 10
  enable_checkpointing: null
  enable_progress_bar: null
  enable_model_summary: null
  accumulate_grad_batches: 1
  gradient_clip_val: 0.5
  gradient_clip_algorithm: null
  deterministic: null
  benchmark: true
  inference_mode: true
  use_distributed_sampler: true
  profiler: null
  #    class_path: PyTorchProfiler
  #    init_args:
  #      dirpath: perf
  #      filename: perf-logs
  detect_anomaly: false
  barebones: false
  plugins: null
  sync_batchnorm: false
  reload_dataloaders_every_n_epochs: 0
  default_root_dir: null
model:
  class_path: src.projects.cogvideox.module.CogVideoX5BActionTransformer
  init_args:
    optimizer:
      class_path: torch.optim.AdamW
      init_args:
        lr: 5e-5
    full_trainable_modules:
      - 'condition_transformer.transformer' # condition transformer
      - 'condition_transformer.condition_proj'
      - 'condition_transformer.vision_proj'
      - 'transformer.transformer_blocks.0.attn1.processor'
      - 'transformer.transformer_blocks.1.attn1.processor'
      - 'transformer.transformer_blocks.2.attn1.processor'
      - 'transformer.transformer_blocks.3.attn1.processor'
      - 'transformer.transformer_blocks.4.attn1.processor'
      - 'transformer.transformer_blocks.5.attn1.processor'
      - 'transformer.transformer_blocks.6.attn1.processor'
      - 'transformer.transformer_blocks.7.attn1.processor'
      - 'transformer.transformer_blocks.8.attn1.processor'
      - 'transformer.transformer_blocks.9.attn1.processor'
      - 'transformer.transformer_blocks.10.attn1.processor'
      - 'transformer.transformer_blocks.11.attn1.processor'
      - 'transformer.transformer_blocks.12.attn1.processor'
      - 'transformer.transformer_blocks.13.attn1.processor'
      - 'transformer.transformer_blocks.14.attn1.processor'
      - 'transformer.transformer_blocks.15.attn1.processor'
      - 'transformer.transformer_blocks.16.attn1.processor'
      - 'transformer.transformer_blocks.17.attn1.processor'
      - 'transformer.transformer_blocks.18.attn1.processor'
      - 'transformer.transformer_blocks.19.attn1.processor'
      - 'transformer.transformer_blocks.20.attn1.processor'
      - 'transformer.transformer_blocks.21.attn1.processor'
      - 'transformer.transformer_blocks.22.attn1.processor'
      - 'transformer.transformer_blocks.23.attn1.processor'
      - 'transformer.transformer_blocks.24.attn1.processor'
      - 'transformer.transformer_blocks.25.attn1.processor'
      - 'transformer.transformer_blocks.26.attn1.processor'
      - 'transformer.transformer_blocks.27.attn1.processor'
      - 'transformer.transformer_blocks.28.attn1.processor'
      - 'transformer.transformer_blocks.29.attn1.processor'
      - 'transformer.transformer_blocks.30.attn1.processor'
      - 'transformer.transformer_blocks.31.attn1.processor'
      - 'transformer.transformer_blocks.32.attn1.processor'
      - 'transformer.transformer_blocks.33.attn1.processor'
      - 'transformer.transformer_blocks.34.attn1.processor'
      - 'transformer.transformer_blocks.35.attn1.processor'
      - 'transformer.transformer_blocks.36.attn1.processor'
      - 'transformer.transformer_blocks.37.attn1.processor'
      - 'transformer.transformer_blocks.38.attn1.processor'
      - 'transformer.transformer_blocks.39.attn1.processor'
      - 'transformer.transformer_blocks.40.attn1.processor'
      - 'transformer.transformer_blocks.41.attn1.processor'

    lora_rank: 256
    lora_trainable_modules: [ ]
    freeze_modules:
      - transformer
      - vae
      - text_encoder
      - condition_transformer
    adapter_modules:
      - 'transformer_blocks.0.attn1.processor'
      - 'transformer_blocks.1.attn1.processor'
      - 'transformer_blocks.2.attn1.processor'
      - 'transformer_blocks.3.attn1.processor'
      - 'transformer_blocks.4.attn1.processor'
      - 'transformer_blocks.5.attn1.processor'
      - 'transformer_blocks.6.attn1.processor'
      - 'transformer_blocks.7.attn1.processor'
      - 'transformer_blocks.8.attn1.processor'
      - 'transformer_blocks.9.attn1.processor'
      - 'transformer_blocks.10.attn1.processor'
      - 'transformer_blocks.11.attn1.processor'
      - 'transformer_blocks.12.attn1.processor'
      - 'transformer_blocks.13.attn1.processor'
      - 'transformer_blocks.14.attn1.processor'
      - 'transformer_blocks.15.attn1.processor'
      - 'transformer_blocks.16.attn1.processor'
      - 'transformer_blocks.17.attn1.processor'
      - 'transformer_blocks.18.attn1.processor'
      - 'transformer_blocks.19.attn1.processor'
      - 'transformer_blocks.20.attn1.processor'
      - 'transformer_blocks.21.attn1.processor'
      - 'transformer_blocks.22.attn1.processor'
      - 'transformer_blocks.23.attn1.processor'
      - 'transformer_blocks.24.attn1.processor'
      - 'transformer_blocks.25.attn1.processor'
      - 'transformer_blocks.26.attn1.processor'
      - 'transformer_blocks.27.attn1.processor'
      - 'transformer_blocks.28.attn1.processor'
      - 'transformer_blocks.29.attn1.processor'
      - 'transformer_blocks.30.attn1.processor'
      - 'transformer_blocks.31.attn1.processor'
      - 'transformer_blocks.32.attn1.processor'
      - 'transformer_blocks.33.attn1.processor'
      - 'transformer_blocks.34.attn1.processor'
      - 'transformer_blocks.35.attn1.processor'
      - 'transformer_blocks.36.attn1.processor'
      - 'transformer_blocks.37.attn1.processor'
      - 'transformer_blocks.38.attn1.processor'
      - 'transformer_blocks.39.attn1.processor'
      - 'transformer_blocks.40.attn1.processor'
      - 'transformer_blocks.41.attn1.processor'
    eval_pipeline_call_kwargs:
      num_inference_steps: 25
      num_frames: 17
      guidance_scale: 3
      sample_method: null
      scheduler: "dpm"

    drop_prob: 0.0
    gradient_checkpointing: false
    ckpt_path: checkpoints/CogVideoX/17_frames.ckpt
    adapter_path: checkpoints/CogVideoX/Motion-Adapter.ckpt

    condition_transformer:
      class_path: src.projects.condition.ActionTransformer
      init_args:
        ckpt_path: checkpoints/CogVideoX/motion_transformer.ckpt
        condition_model:
          class_path: src.projects.condition.DINOImageEmbedder
          init_args:
            model: facebook/dinov2-large
            freeze: true

        condition_proj:
          class_path: src.projects.condition.Resampler
          init_args:
            dim: 1024
            depth: 4
            dim_head: 64
            heads: 12
            num_queries: 25
            embedding_dim: 1024
            output_dim: 1024
            ff_mult: 4

        vision_model:
          class_path: src.projects.condition.VideoMAEEmbedder
          init_args:
            freeze: true

        vision_proj:
          class_path: src.projects.condition.Resampler
          init_args:
            dim: 1024
            depth: 4
            dim_head: 64
            heads: 12
            num_queries: 25
            embedding_dim: 768
            output_dim: 1024
            ff_mult: 4
            ckpt_path: checkpoints/CogVideoX/motion_proj.ckpt # 25 token

        condition_pe:
          class_path: src.projects.condition.SinusoidPositionalEmbeddings
          init_args:
            dim: 1024
            max_length: 2560

        vision_pe:
          class_path: src.projects.condition.SinusoidPositionalEmbeddings
          init_args:
            dim: 1024
            max_length: 256

        transformer:
          class_path: torch.nn.TransformerEncoder
          init_args:
            num_layers: 4
            encoder_layer:
              class_path: torch.nn.TransformerEncoderLayer
              init_args:
                d_model: 1024
                nhead: 16
                dim_feedforward: 4096
                dropout: 0.0
                activation: gelu
                batch_first: true
                norm_first: false
                bias: true

data:
  class_path: src.data.VideoDataModule
  init_args:
    video_size:
      - 480
      - 720
    video_length: 17
    # probability of dropping the text prompt
    uncond_text_ratio: 0.15
    uncond_video_ratio: 0.15
    prompt_type: llm
    use_ref_frame: false
    train_annotation_path:
      - "datasets/OpenVid-1M/data/openvid-1m.parquet"
    val_annotation_path:
      - "datasets/OpenVid-1M/data/openvid-1k.parquet"
    test_annotation_path:
      - "datasets/OpenVid-1M/data/openvid-1k.parquet"
    video_dir: datasets
    train_transforms: [ ]
    train_clip_selector:
      class_path: src.data.clip_selector.AllSelector

    val_clip_selector:
      class_path: src.data.clip_selector.RandomSelector
      init_args:
        num_clips: 128
        seed: 42
    test_clip_selector:
      class_path: src.data.clip_selector.AllSelector

    num_workers: 3
    prefetch_factor: 5
    train_batch_size: 1
    val_batch_size: 1
    test_batch_size: 1
    collate_fn: src.data.dataset.collate_fn
    tokenizer_model: null
    sampling_config: # video sampling rate (fps) and its probability, i.e. {4: 0.2, 8: 0.5, 12: 0.3}
      8: 1.0
    ref_video_type: "rag_text"
    rag_prompt_type: motion
    rag_db_path: datasets/rag/openvid.db
    ref_video_num: 9

#ckpt_path: null
